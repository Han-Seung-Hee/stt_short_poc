# Air-Gapped STT + 요약 PoC — M1 Mac 실행 계획

폐쇄망 M1 Mac(16GB)에서 WAV 음성파일을 텍스트로 변환(STT)하고, 3줄 요약을 반환하는 FastAPI 기반 오프라인 AI 서비스 PoC를 구축합니다. 향후 Linux CPU 서버로의 확장을 고려하여 추상화 레이어로 설계합니다.

## 환경 요약

| 항목 | 값 |
|------|-----|
| 타겟 HW | Apple M1 Mac, 16GB Unified Memory |
| STT | `mlx-whisper` (Whisper `small` 모델, ~462MB) |
| LLM | Ollama + `qwen2.5:7b` (~4.7GB) |
| API | FastAPI + Uvicorn |
| 음성 입력 | WAV, 5분 이내, 16kHz mono 가정 |
| 청크 분할 | 옵션 (ON/OFF, 기본 OFF) |

> [!IMPORTANT]
> 16GB M1에서 STT(~1GB) + LLM(~7GB) 합산 약 8GB는 동시 구동 가능 범위이나, 안전을 위해 **순차 처리**(STT 완료 → LLM 호출) 방식을 기본으로 합니다.

## User Review Required

> [!WARNING]
> **Whisper 모델 크기 결정**: 5분 이내 음성 기준, `small` 모델(~462MB, 한국어 품질 양호)을 기본으로 합니다. 한국어 인식 정확도가 부족하면 `medium`(~1.5GB)으로 교체 가능하도록 설정 파일에서 변경할 수 있게 합니다. 이 선택에 동의하시나요?

> [!NOTE]
> **Linux 확장 전략**: 향후 GPU 없는 Linux 서버에서는 `mlx-whisper` 대신 `faster-whisper`(CPU 모드)로, Ollama는 그대로 사용합니다. 이를 위해 STT 엔진을 인터페이스로 추상화합니다.

---

## Proposed Changes

### 프로젝트 구조

```
d:\dev\stt_short\
├── app/
│   ├── __init__.py
│   ├── main.py              # FastAPI 앱 진입점
│   ├── config.py             # 설정 관리 (환경변수 + YAML)
│   ├── stt/
│   │   ├── __init__.py
│   │   ├── base.py           # STT 추상 인터페이스
│   │   ├── mlx_engine.py     # mlx-whisper 구현 (Mac)
│   │   └── faster_engine.py  # faster-whisper 구현 (Linux, 스텁)
│   ├── llm/
│   │   ├── __init__.py
│   │   ├── base.py           # LLM 추상 인터페이스
│   │   └── ollama_engine.py  # Ollama 구현
│   └── pipeline.py           # STT → 요약 통합 파이프라인
├── config.yaml               # 기본 설정 파일
├── requirements.txt          # Python 의존성
├── offline_setup_guide.md    # 오프라인 설치 가이드 문서
└── README.md                 # 프로젝트 설명
```

---

### STT 추상화 레이어

#### [NEW] [base.py](file:///d:/dev/stt_short/app/stt/base.py)
- `STTEngine` ABC: `transcribe(audio_path, language, chunk_enabled, chunk_length_sec)` → `STTResult`
- `STTResult` dataclass: `text`, `segments[]`, `language`, `duration_sec`

#### [NEW] [mlx_engine.py](file:///d:/dev/stt_short/app/stt/mlx_engine.py)
- `MLXWhisperEngine(STTEngine)`: `mlx_whisper.transcribe()` 호출
- 모델 경로를 config에서 읽어 로컬 모델 파일 사용
- 청크 옵션: 활성 시 `pydub`로 WAV를 N초 단위로 분리 후 순차 처리, 결과 병합

#### [NEW] [faster_engine.py](file:///d:/dev/stt_short/app/stt/faster_engine.py)
- `FasterWhisperEngine(STTEngine)`: 향후 Linux 확장용 스텁 (인터페이스만 구현, NotImplementedError)

---

### LLM 요약 추상화 레이어

#### [NEW] [base.py](file:///d:/dev/stt_short/app/llm/base.py)
- `LLMEngine` ABC: `summarize(text, system_prompt)` → `SummaryResult`
- `SummaryResult` dataclass: `summary`, `model_name`, `token_count`

#### [NEW] [ollama_engine.py](file:///d:/dev/stt_short/app/llm/ollama_engine.py)
- `OllamaEngine(LLMEngine)`: `httpx`로 Ollama REST API(`POST /api/generate`) 호출
- 한국어 3줄 요약 시스템 프롬프트 내장
- 타임아웃, 재시도 처리

---

### FastAPI 서버

#### [NEW] [main.py](file:///d:/dev/stt_short/app/main.py)
- `POST /api/v1/transcribe` — WAV 업로드 → STT 결과만 반환
- `POST /api/v1/summarize` — 텍스트 입력 → 3줄 요약 반환
- `POST /api/v1/process` — WAV 업로드 → STT + 요약 통합 반환 (핵심 엔드포인트)
- `GET /api/v1/health` — 헬스체크
- JSON 응답: `{ transcript, summary, duration_sec, model_info }`

#### [NEW] [config.py](file:///d:/dev/stt_short/app/config.py)
- Pydantic Settings: `config.yaml` + 환경변수 오버라이드
- STT 엔진 종류, 모델 경로, 청크 설정, Ollama URL/모델명 등

#### [NEW] [pipeline.py](file:///d:/dev/stt_short/app/pipeline.py)
- `ProcessingPipeline`: STT → LLM을 순차 호출하는 오케스트레이터
- 메모리 관리: STT 완료 후 명시적 GC 힌트

---

### 설정 및 문서

#### [NEW] [config.yaml](file:///d:/dev/stt_short/config.yaml)
```yaml
stt:
  engine: "mlx"          # "mlx" | "faster" (Linux)
  model_name: "small"    # tiny, base, small, medium, large-v3
  language: "ko"
  chunk:
    enabled: false
    length_sec: 300       # 5분 단위 청크
llm:
  engine: "ollama"
  base_url: "http://localhost:11434"
  model_name: "qwen2.5:7b"
  timeout_sec: 120
server:
  host: "0.0.0.0"
  port: 8000
  max_file_size_mb: 100
```

#### [NEW] [requirements.txt](file:///d:/dev/stt_short/requirements.txt)
- Mac: `mlx-whisper`, `mlx`, `fastapi`, `uvicorn`, `pydub`, `httpx`, `pyyaml`, `pydantic-settings`, `python-multipart`
- Linux(향후): `faster-whisper`, `ctranslate2` 로 교체

#### [NEW] [offline_setup_guide.md](file:///d:/dev/stt_short/offline_setup_guide.md)
- **외부망 준비**: python.org 설치파일, pip wheel 다운로드, Ollama .dmg, Whisper 모델 다운로드, GGUF LLM 모델 다운로드 — 구체적 커맨드 포함
- **내부망 설치**: Xcode CLT 설치, Python 설치, venv 생성, wheel 오프라인 설치, Ollama 설치/모델 로드 — 구체적 절차
- Homebrew 불사용

#### [NEW] [README.md](file:///d:/dev/stt_short/README.md)
- 프로젝트 개요, 퀵스타트, API 사용법, 메모리 관리 팁

---

## 메모리 관리 전략 (16GB Unified Memory)

| 단계 | 메모리 점유 | 비고 |
|------|-----------|------|
| Idle | ~2GB (OS+Python) | — |
| STT 실행 | +~1GB (small) / +~1.5GB (medium) | MLX Metal 가속 |
| STT 완료 → GC | -STT 모델 해제 가능 | `gc.collect()` 호출 |
| LLM 호출 | +~7GB (Ollama 상시 로드) | Ollama 데몬 별도 프로세스 |
| **최대 동시** | **~10GB** | 16GB 내 안전 범위 |

- **방침**: STT 모델은 요청 시 로드 → 사용 → 해제 (lazy loading) 또는 상주 중 택일 (설정)
- **Ollama**: 별도 프로세스이므로 Python과 메모리 경합 낮음. `OLLAMA_NUM_PARALLEL=1` 권장

---

## Verification Plan

### Automated Tests
이 프로젝트는 PoC이며, 외부 모델(Ollama, mlx-whisper)에 의존하므로 실제 동작 테스트는 M1 Mac 현장에서 수행해야 합니다. 코드 수준에서는 아래를 검증합니다:

1. **Python 구문 검증**: `python -m py_compile app/main.py` 등 모든 파일에 대해 컴파일 체크
   ```
   cd d:\dev\stt_short
   python -m py_compile app/main.py
   python -m py_compile app/config.py
   python -m py_compile app/pipeline.py
   python -m py_compile app/stt/base.py
   python -m py_compile app/stt/mlx_engine.py
   python -m py_compile app/llm/base.py
   python -m py_compile app/llm/ollama_engine.py
   ```

2. **Import 체크**: FastAPI 앱이 정상 임포트되는지 확인
   ```
   cd d:\dev\stt_short
   python -c "from app.main import app; print('Import OK')"
   ```

### Manual Verification (M1 Mac 현장)
사용자가 M1 Mac에서 실제 테스트할 때 사용할 절차:

1. `offline_setup_guide.md`의 절차에 따라 환경 구성
2. Ollama 실행 및 모델 로드 확인: `ollama list`
3. FastAPI 서버 시작: `uvicorn app.main:app --host 0.0.0.0 --port 8000`
4. 헬스체크: `curl http://localhost:8000/api/v1/health`
5. 테스트 WAV 파일로 통합 처리 테스트:
   ```
   curl -X POST http://localhost:8000/api/v1/process \
     -F "file=@test_audio.wav" \
     -F "chunk_enabled=false"
   ```
6. 응답 JSON에 `transcript`, `summary` 필드가 정상 반환되는지 확인
