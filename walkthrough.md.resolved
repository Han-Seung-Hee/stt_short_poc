# Walkthrough: Air-Gapped STT + 요약 PoC

폐쇄망 M1 Mac(16GB)용 음성→텍스트→3줄 요약 API 서비스를 구현했습니다.

## 구현 결과

### 아키텍처 다이어그램

```mermaid
graph LR
    A["WAV Upload"] --> B["FastAPI Server<br/>:8000"]
    B --> C["STT Engine<br/>(추상 인터페이스)"]
    C -->|Mac| D["mlx-whisper<br/>Metal 가속"]
    C -->|Linux| E["faster-whisper<br/>CPU 모드"]
    D --> F["Pipeline<br/>GC 수행"]
    E --> F
    F --> G["LLM Engine<br/>(추상 인터페이스)"]
    G --> H["Ollama<br/>REST API"]
    H --> I["JSON Response<br/>transcript + summary"]
```

### 생성된 파일 (11개 Python + 4개 문서/설정)

| 파일 | 역할 |
|------|------|
| [main.py](file:///d:/dev/stt_short/app/main.py) | FastAPI 서버 (4개 엔드포인트) |
| [config.py](file:///d:/dev/stt_short/app/config.py) | YAML + 환경변수 설정 관리 |
| [pipeline.py](file:///d:/dev/stt_short/app/pipeline.py) | STT→LLM 순차 오케스트레이션 |
| [stt/base.py](file:///d:/dev/stt_short/app/stt/base.py) | STT 추상 인터페이스 |
| [stt/mlx_engine.py](file:///d:/dev/stt_short/app/stt/mlx_engine.py) | mlx-whisper 구현 (청크 지원) |
| [stt/faster_engine.py](file:///d:/dev/stt_short/app/stt/faster_engine.py) | faster-whisper 구현 (Linux용) |
| [llm/base.py](file:///d:/dev/stt_short/app/llm/base.py) | LLM 추상 인터페이스 |
| [llm/ollama_engine.py](file:///d:/dev/stt_short/app/llm/ollama_engine.py) | Ollama REST API 구현 |
| [config.yaml](file:///d:/dev/stt_short/config.yaml) | 기본 설정 파일 |
| [requirements.txt](file:///d:/dev/stt_short/requirements.txt) | Python 의존성 |
| [offline_setup_guide.md](file:///d:/dev/stt_short/offline_setup_guide.md) | 오프라인 설치 가이드 |
| [README.md](file:///d:/dev/stt_short/README.md) | 프로젝트 문서 |

## 핵심 설계 결정

### 1. STT/LLM 추상화 레이어
- [STTEngine](file:///d:/dev/stt_short/app/stt/base.py#29-65) ABC → [MLXWhisperEngine](file:///d:/dev/stt_short/app/stt/mlx_engine.py#19-199) (Mac) / [FasterWhisperEngine](file:///d:/dev/stt_short/app/stt/faster_engine.py#16-110) (Linux)
- [LLMEngine](file:///d:/dev/stt_short/app/llm/base.py#21-53) ABC → [OllamaEngine](file:///d:/dev/stt_short/app/llm/ollama_engine.py#29-173) (양쪽 동일)
- [config.yaml](file:///d:/dev/stt_short/config.yaml)에서 `stt.engine: "mlx"` ↔ `"faster"` 전환만으로 플랫폼 교체

### 2. 청크 분할 (옵션)
- [config.yaml](file:///d:/dev/stt_short/config.yaml)의 `stt.chunk.enabled`로 글로벌 ON/OFF
- API 호출 시 `chunk_enabled` 파라미터로 요청별 오버라이드 가능
- pydub로 WAV 분할 → 순차 STT → 시간 오프셋 보정 후 병합

### 3. 메모리 관리
- STT→LLM 순차 처리 (동시 로드 회피)
- STT 완료 후 `gc.collect()` 호출
- Ollama는 별도 프로세스 (Python 메모리와 격리)

## 검증 결과

| 항목 | 결과 |
|------|------|
| Python AST 파싱 (11개 파일) | ✅ 전체 통과 |
| 프로젝트 구조 | ✅ 계획대로 생성 |
| 오프라인 가이드 | ✅ Homebrew 미사용, python.org + pip wheel 기반 |

> [!NOTE]
> **실제 동작 테스트**는 M1 Mac 현장에서 [offline_setup_guide.md](file:///d:/dev/stt_short/offline_setup_guide.md)에 따라 환경을 구성한 후 수행해야 합니다. Windows 환경에서는 `mlx-whisper`가 동작하지 않으므로 코드 구문 검증까지만 수행했습니다.

## M1 Mac에서 실행할 때 순서

```bash
# 1. Ollama 실행
open /Applications/Ollama.app

# 2. 가상환경 활성화
source .venv/bin/activate

# 3. config.yaml에서 model_path 설정
# stt.model_path: "/path/to/whisper-small-mlx"

# 4. 서버 시작
uvicorn app.main:app --host 0.0.0.0 --port 8000

# 5. 테스트
curl -X POST http://localhost:8000/api/v1/process \
  -F "file=@test.wav"
```
